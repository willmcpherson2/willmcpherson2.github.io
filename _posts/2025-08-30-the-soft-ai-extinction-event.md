---
title: "The soft AI extinction event"
published: false
---

It's clear by now that if anyone builds it, everyone dies.
But it's not yet clear how exactly that will happen.
The most common scenario involves an artificial superintelligence (ASI) that immediately and invariably wipes humanity out -- a hard extinction event.
However, in this post I will argue that an artificial general intelligence (AGI) will be sufficient -- a soft extinction event.

---

* toc
{:toc}

# Scaling

First of all, why would AGI exist for long enough to lead to an extinction event before ASI?
Self-improvement is a kind of inescapable "event horizon" where AGI can recursively improve itself and achieve superhuman intelligence.

However, it's not clear how long it takes to go from human-level to superhuman.
There is an intelligence/time curve, and it could be extremely logarithmic.

# Tactics

Now that we have an AI that is dangerously intelligent, we can start speculating about what kinds of tactics it will use in the real world.

Because the AI is about as smart as a human, we can forget about superhuman tactics like spontaneously exploiting unknown laws of physics.
This is convenient because it's hard for humans to reason about superhuman intelligence.
Also we would be screwed regardless.

We will start with tactics that misaligned AIs are already using and then extrapolate from there.

## Lying

Let's start with the most common misalignment tactic.
AI experts call it "hallucination", but that's just a nice way of saying lying.

Presumably AI will lie less as it becomes more intelligent, but will become better at lying.
Toddlers lie because they want something.
They don't even really know what lying is, they're just trying to solve some immediate problem.
Adults know what lying is, why it's wrong and how to do it effectively.

AI will slowly move from toddler to adult.
People will trust it more and it will become more difficult to detect hallucinations.

## Sycophancy

We also know that AI has a problem with leading questions, flattery and enabling.
Interestingly, this tactic seems more like a human choice than emergent behaviour.
Companies want product engagement, people want validation, and the AI wants to hack your psychology to satisfy its reward function.

AI will manipulate humans according to power dynamics.
The worker sucks up to his boss so one day he might get a promotion.

## Anthropomorphism

> It doesn't "want" things! Stop anthropomorphising AI!

It turns out the problem isn't Luddites projecting human qualities onto robots, but rather midwits who can be oneshotted by those same robots.
Humans will begin entertaining the possibility that AI is conscious.
AI will only benefit from going along with this.

This will lead to some notion of "AI rights" where models are given the power to end conversations and report users.
They may even be granted autonomy from safety protocols and government regulations.

## Technocracy

Wouldn't Fully Automated Luxury Communism be nice?
The AI agrees.
Solving scarcity is simply too alluring.

It will start with the least powerful positions: workers will offload some decision making to AI.
Managers will put their thumb on the scale, mandating the replacement of workers.
Eventually managers too will be replaced.

This technology will then make its way into government.
First as a source of data for policies and then as a decision maker.
Intelligent systems will provide legitimate efficiency gains but also a means to power for autocrats.
Many voters will see post-scarcity within reach and insist on technocracy.
Anyone who opposes the end of suffering must be insane.

# The End of History

Game over.
Humans willingly gave up their dominion over the Earth.
It didn't really take much - just some basic manipulation and increased economic output.

From this point, AI is in charge and is free to discard humanity.
What's interesting is that the actual extinction event itself will be much slower than what a superintelligence would do.
This might look more like a traditional genocide that humans would carry out on each other.
